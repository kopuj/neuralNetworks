{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 6. Transformer models for text processing\n",
    "\n",
    "In the previous documents, we have looked at several different neural network models for classifying movie reviews according to their sentiment (positive/negative). Now that we are acquainted with all the necessary building blocks, we are ready to assemble a complete Transformer model. "
   ],
   "id": "21416e736aebd471"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T06:43:25.107609Z",
     "start_time": "2024-11-27T06:43:06.843890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras import ops\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ],
   "id": "2b782fee45d6f1e7",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Text classification (sentiment analysis)\n",
    "\n",
    "For simplicity, we keep using the same (by now very familiar) IMDB dataset:"
   ],
   "id": "56fd7270b5f53272"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-27T06:43:41.546724Z",
     "start_time": "2024-11-27T06:43:36.012441Z"
    }
   },
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_ds = keras.utils.text_dataset_from_directory( \n",
    "    '../../aclImdb/train/', \n",
    "    validation_split=0.2, \n",
    "    subset=\"training\", \n",
    "    seed=123,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "val_ds = keras.utils.text_dataset_from_directory( \n",
    "    '../../aclImdb/train/', \n",
    "    validation_split=0.2, \n",
    "    subset=\"validation\", \n",
    "    seed=123, # same seed as above!\n",
    "    batch_size=batch_size)\n",
    "\n",
    "test_ds = keras.utils.text_dataset_from_directory( \n",
    "    '../../aclImdb/test/', \n",
    "    batch_size=batch_size)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As before, we preprocess the original string-valued samples to integer-valued token sequences of fixed length:",
   "id": "af853db2329cdea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T06:43:52.596256Z",
     "start_time": "2024-11-27T06:43:46.802683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab_size = 10000 # maximum number of tokens in vocabulary \n",
    "sequence_length = 250 # maximum length of sequences\n",
    "\n",
    "vectorization_layer = layers.TextVectorization( \n",
    "    max_tokens=vocab_size, \n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "train_texts = train_ds.map(lambda x, y: x) \n",
    "vectorization_layer.adapt(train_texts)\n",
    "\n",
    "train_ds_int = train_ds.map(lambda x, y: (vectorization_layer(x), y)) \n",
    "val_ds_int = val_ds.map(lambda x, y: (vectorization_layer(x), y))\n",
    "test_ds_int = test_ds.map(lambda x, y: (vectorization_layer(x), y))"
   ],
   "id": "fb2234636d949219",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we define a custom layer for converting the integer lists to word vectors. This layer consists of two separate Embedding layers: one encoding the tokens, and one encoding their positions in the sample. The final embeddings are obtained by summing these two separate contributions. Note that we also include the ability to include a mask for ignoring the padding zeroes in the input samples, as well as a `get_config` method to allow for saving the model after training. ",
   "id": "f6a1a30c383ff135"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T06:46:22.501973Z",
     "start_time": "2024-11-27T06:46:22.493957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = ops.shape(inputs)[-1]\n",
    "        positions = ops.arange(0, length, 1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return ops.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"sequence_length\": self.sequence_length,\n",
    "                \"vocab_size\": self.vocab_size,\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "            }\n",
    "        )\n",
    "        return config"
   ],
   "id": "a00020f9d1740df0",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We are now ready to build the Transformer model; the structure of the original model is presented in the image below (Yuening Jia, CC BY-SA 3.0 <https://creativecommons.org/licenses/by-sa/3.0>, via Wikimedia Commons):",
   "id": "ee08e5154789829a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T06:46:28.206726Z",
     "start_time": "2024-11-27T06:46:28.186458Z"
    }
   },
   "cell_type": "code",
   "source": "from IPython.display import Image,display;display(Image(filename=\"Images/The-Transformer-model-architecture.png\"))",
   "id": "94d3bcf228de7bfb",
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For the purpose of our classification model, we shall employ the **encoder** part of the architecture, which constitutes the left-hand part in the image above. First, the embedded input samples enter the multi-head attention layer, whose output is then summed with the original input coming through a residual connection. Following a normalization, the tensor enters a fully connected segment containing two Dense layers. The output from this dense projection is then added to the input tensor via a residual connection, and normalized once more to produce the final output of the Transformer encoder.",
   "id": "f3304f680edc0de6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T06:47:11.506444Z",
     "start_time": "2024-11-27T06:47:11.492194Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(dense_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            padding_mask = ops.cast(mask[:, None, :], dtype=\"int32\")\n",
    "        else:\n",
    "            padding_mask = None\n",
    "\n",
    "        attention_output = self.attention(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
    "        )\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "                \"dense_dim\": self.dense_dim,\n",
    "                \"num_heads\": self.num_heads,\n",
    "            }\n",
    "        )\n",
    "        return config\n"
   ],
   "id": "b258aa4d3d86c12d",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We are now ready to build the actual classifier model. The output from the encoder is flattened by a global pooling layer, and then fed straight into the output layer of a single neuron with sigmoid activation.",
   "id": "5bf1eb515995b216"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T06:47:17.470964Z",
     "start_time": "2024-11-27T06:47:16.179529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embed_dim = 32 # dimension of word embeddings (token + pos)\n",
    "dense_dim = 64 # \n",
    "num_heads = 2\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(), \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ],
   "id": "8ecae66d00669c53",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kopuj\\Anaconda3\\envs\\keras-cpu\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:204: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"functional_1\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape     \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m   Param #\u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mConnected to     \u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m)      │          \u001B[38;5;34m0\u001B[0m │ -                 │\n",
       "│ (\u001B[38;5;33mInputLayer\u001B[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ positional_embeddi… │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m32\u001B[0m)  │    \u001B[38;5;34m328,000\u001B[0m │ input_layer[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m] │\n",
       "│ (\u001B[38;5;33mPositionalEmbeddi…\u001B[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m)      │          \u001B[38;5;34m0\u001B[0m │ input_layer[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m] │\n",
       "│ (\u001B[38;5;33mNotEqual\u001B[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_encoder │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m32\u001B[0m)  │     \u001B[38;5;34m12,736\u001B[0m │ positional_embed… │\n",
       "│ (\u001B[38;5;33mTransformerEncode…\u001B[0m │                   │            │ not_equal[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m32\u001B[0m)        │          \u001B[38;5;34m0\u001B[0m │ transformer_enco… │\n",
       "│ (\u001B[38;5;33mGlobalAveragePool…\u001B[0m │                   │            │ not_equal[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001B[38;5;33mDropout\u001B[0m) │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m32\u001B[0m)        │          \u001B[38;5;34m0\u001B[0m │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001B[38;5;33mDense\u001B[0m)     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)         │         \u001B[38;5;34m33\u001B[0m │ dropout_1[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ positional_embeddi… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">328,000</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbeddi…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_encoder │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,736</span> │ positional_embed… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ transformer_enco… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m340,769\u001B[0m (1.30 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">340,769</span> (1.30 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m340,769\u001B[0m (1.30 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">340,769</span> (1.30 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Train the model ...",
   "id": "dc117025b5a0faab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T06:48:28.443991Z",
     "start_time": "2024-11-27T06:47:28.776736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "history = model.fit(\n",
    "    train_ds_int, \n",
    "    batch_size=batch_size, \n",
    "    epochs=3, \n",
    "    validation_data=(val_ds_int)\n",
    ")\n"
   ],
   "id": "d0bd0b59acca4b53",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kopuj\\Anaconda3\\envs\\keras-cpu\\Lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kopuj\\Anaconda3\\envs\\keras-cpu\\Lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kopuj\\Anaconda3\\envs\\keras-cpu\\Lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m625/625\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m22s\u001B[0m 32ms/step - accuracy: 0.6951 - loss: 0.5571 - val_accuracy: 0.7462 - val_loss: 0.5242\n",
      "Epoch 2/3\n",
      "\u001B[1m625/625\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m19s\u001B[0m 30ms/step - accuracy: 0.8637 - loss: 0.3234 - val_accuracy: 0.8492 - val_loss: 0.3606\n",
      "Epoch 3/3\n",
      "\u001B[1m625/625\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m19s\u001B[0m 31ms/step - accuracy: 0.8897 - loss: 0.2749 - val_accuracy: 0.8740 - val_loss: 0.2977\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "... and test it.",
   "id": "d1a6a3f6a109812e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T10:44:18.118193Z",
     "start_time": "2024-11-27T10:44:06.213388Z"
    }
   },
   "cell_type": "code",
   "source": "model.evaluate(test_ds_int)",
   "id": "f3c097972c6aea9a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m782/782\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 15ms/step - accuracy: 0.8656 - loss: 0.3205\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.31755366921424866, 0.8659600019454956]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Text generation\n",
    "\n",
    "In addition to simple tasks like classification, Transformer architecture models can also be used for more ambitious natural language processing tasks, such as machine translation and text generation. Here we shall take a look at how a neural network model can be trained to produce movie reviews from an initial prompt. \n",
    "\n",
    "We begin by creating a new Dataset object from the data in the training directory only. This time, we do not need the sentiment labels, and remove the line break tokens in order to avoid generating those later."
   ],
   "id": "3cb96f236d7cd3fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T10:44:29.467149Z",
     "start_time": "2024-11-27T10:44:27.069802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = keras.utils.text_dataset_from_directory(\n",
    "    directory=\"../../aclImdb/train/\", \n",
    "    label_mode=None, \n",
    "    batch_size=256)\n",
    "\n",
    "dataset = dataset.map(lambda x: tf.strings.regex_replace(x, \"<br />\", \"\"))"
   ],
   "id": "24c15dc8aa7bf7f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, we convert the strings to integer lists, as before. To speed up training, we restrict the sequence lengths to be somewhat shorter than before.",
   "id": "4a8e929d6bae3c27"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T10:44:38.539550Z",
     "start_time": "2024-11-27T10:44:33.610148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sequence_length = 100\n",
    "vocab_size = 10000\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "text_vectorization.adapt(dataset)"
   ],
   "id": "3d7856bb7ce47196",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, we generate specialized targets for the purpose of training the model: the targets are simply the same integer sequences as the samples, but shifted one token to the right (this means that the sequence length gets reduced by one token from the original ones). ",
   "id": "36aa3103da705b07"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T10:44:40.918553Z",
     "start_time": "2024-11-27T10:44:40.488990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_lm_dataset(text_batch):\n",
    "    vectorized_sequences = text_vectorization(text_batch)\n",
    "    x = vectorized_sequences[:, :-1]\n",
    "    y = vectorized_sequences[:, 1:]\n",
    "    return x, y\n",
    "\n",
    "lm_dataset = dataset.map(prepare_lm_dataset)"
   ],
   "id": "f9841b801b257f8c",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For the actual text generation, we need to employ the **decoder** part of the Transformer (the right-hand side of the architecture image above). The building blocks of the decoder are very similar to those of the encoder, but with a couple of differences. First, there are two separate attention layers; the first layer takes the embedded inputs in as query, key and value. The second attention layer takes the output of the first one in as the query. In the case of a machine translation task the key and value would come from in from the encoder outputs generated by the source sequence to be translated; here, however, there is no such separate source sequence, but the keys and values are provided by the original inputs to the decoder.\n",
    "\n",
    "Another important detail is the **causal mask**, which prevents the generated from attending to future words, when predicting a given word in a sequence. The implementation of this, as well as all the other essentials of the code are from F. Chollet: **Deep Learning with Python** (Chapter 12), with migration guidelines to Keras 3 in [this example](https://keras.io/examples/nlp/neural_machine_translation_with_transformer/)."
   ],
   "id": "e5f17086db0e2ea1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T10:44:46.092223Z",
     "start_time": "2024-11-27T10:44:46.059443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(latent_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = ops.cast(mask[:, None, :], dtype=\"int32\")\n",
    "            padding_mask = ops.minimum(padding_mask, causal_mask)\n",
    "        else:\n",
    "            padding_mask = None\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(out_2 + proj_output)\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = ops.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = ops.arange(sequence_length)[:, None]\n",
    "        j = ops.arange(sequence_length)\n",
    "        mask = ops.cast(i >= j, dtype=\"int32\")\n",
    "        mask = ops.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = ops.concatenate(\n",
    "            [ops.expand_dims(batch_size, -1), ops.convert_to_tensor([1, 1])],\n",
    "            axis=0,\n",
    "        )\n",
    "        return ops.tile(mask, mult)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "                \"latent_dim\": self.latent_dim,\n",
    "                \"num_heads\": self.num_heads,\n",
    "            }\n",
    "        )\n",
    "        return config"
   ],
   "id": "400ba417d38ab6f2",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "3df06d1bab9a625b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We are now ready to build the model with the Transformer decoder. Its output is directly connected to the output layer with the size of the vocabulary and softmax activation, to provide a probability distribution for token predictions. Note that we choose `sparse_categorical_crossentropy` as our loss function, because the target labels consist of integers (instead of being one-hot encoded).",
   "id": "d486ccb406b67d7d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T10:44:51.223609Z",
     "start_time": "2024-11-27T10:44:50.683999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embed_dim = 32\n",
    "latent_dim = 64\n",
    "num_heads = 2\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
    "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, x)\n",
    "outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "              optimizer=keras.optimizers.RMSprop())\n",
    "\n",
    "model.summary()"
   ],
   "id": "1b51e72262245b76",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"functional_3\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape     \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m   Param #\u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mConnected to     \u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m)      │          \u001B[38;5;34m0\u001B[0m │ -                 │\n",
       "│ (\u001B[38;5;33mInputLayer\u001B[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ positional_embeddi… │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m32\u001B[0m)  │    \u001B[38;5;34m323,200\u001B[0m │ input_layer_2[\u001B[38;5;34m0\u001B[0m]… │\n",
       "│ (\u001B[38;5;33mPositionalEmbeddi…\u001B[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_decoder │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m32\u001B[0m)  │     \u001B[38;5;34m21,216\u001B[0m │ positional_embed… │\n",
       "│ (\u001B[38;5;33mTransformerDecode…\u001B[0m │                   │            │ positional_embed… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (\u001B[38;5;33mDense\u001B[0m)     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m,      │    \u001B[38;5;34m330,000\u001B[0m │ transformer_deco… │\n",
       "│                     │ \u001B[38;5;34m10000\u001B[0m)            │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ positional_embeddi… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">323,200</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbeddi…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_decoder │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">21,216</span> │ positional_embed… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ positional_embed… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">330,000</span> │ transformer_deco… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">10000</span>)            │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m674,416\u001B[0m (2.57 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">674,416</span> (2.57 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m674,416\u001B[0m (2.57 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">674,416</span> (2.57 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we can train the model.",
   "id": "a234182d70731a37"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T10:46:15.708645Z",
     "start_time": "2024-11-27T10:45:03.846833Z"
    }
   },
   "cell_type": "code",
   "source": "model.fit(lm_dataset, epochs=1)",
   "id": "f65d340549ae25e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m98/98\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m72s\u001B[0m 697ms/step - loss: 8.3777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1b91aa119a0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Once the model has been trained, we can experiment with generating text from an initial seed prompt. Instead of always predicting the token with the highest probability in the predicted distribution, we scale the distribution with a parameter referred to as **temperature**: high values of temperature tend to flatten the probability distribution, which leads to somewhat more surprising choices for next tokens.",
   "id": "721ade4e1cf77948"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T10:46:19.733455Z",
     "start_time": "2024-11-27T10:46:19.712675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokens_index = dict(enumerate(text_vectorization.get_vocabulary()))\n",
    "\n",
    "def sample_next(predictions, temperature=1.0):\n",
    "    predictions = np.asarray(predictions).astype(\"float64\")\n",
    "    predictions = np.log(predictions) / temperature\n",
    "    exp_preds = np.exp(predictions)\n",
    "    predictions = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, predictions, 1)\n",
    "    return np.argmax(probas)"
   ],
   "id": "81505246136ec361",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finally, we can define an initial prompt, and check out the quality of generated text. Unfortunately, due to the short training time, the results are fairly disappointing. ",
   "id": "3ced59f70f9d0ac1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T10:46:25.274144Z",
     "start_time": "2024-11-27T10:46:23.488931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "temperature = 0.5\n",
    "\n",
    "sentence = \"in my view\"\n",
    "generate_length = 50\n",
    "for i in range(generate_length):\n",
    "    tokenized_sentence = text_vectorization([sentence])\n",
    "    predictions = model(tokenized_sentence)\n",
    "    next_token = sample_next(predictions[0, i, :])\n",
    "    sampled_token = tokens_index[next_token]\n",
    "    sentence += \" \" + sampled_token\n",
    "print(sentence)"
   ],
   "id": "dab069e1cca9bc04",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in my view [UNK] dan loser best doesnt a shaped videos [UNK] reckless opposite believe favourite definitely was are wont twodimensional were bad kind writer hours never alternately other ensues minions subsequently use me seen an intent a dvd pulling even mystery tripe any dance more hartleys thrown justified increasingly this canada former\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ccbfd75b84aea31b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
