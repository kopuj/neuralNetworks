{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4. Text data and preprocessing\n",
    "\n",
    "Most of the recent impressive advances in machine learning and artificial intelligence have emerged in the field of natural language processing. In addition to simpler tasks such as sentiment analysis or text classification, there has been considerable progress in more advanced tasks such as translation and question answering. Text processing involves several unique techniques and methods, which we shall look at next.\n",
    "\n",
    "## Bag-of-words approach\n",
    "\n",
    "In natural language processing tasks, the original data is typically in the form of strings, or a list of strings. Neural networks cannot process strings directly, so they need to be converted to arrays consisting of numerical values. This preprocessing usually consists of a succession of distinct steps:\n",
    "\n",
    "* *Text standardization*: convert everything to lowercase, remove punctuation and other special characters, etc.\n",
    "* *Tokenization*: split the text into separate units or **tokens** (words, characters, N-grams ...), and set up a vocabulary\n",
    "* *Vectorization*: associate a numerical vector with each of the tokens in the vocabulary\n",
    "\n",
    "To investigate an example of implementing these methods, we use the well-known IMDB movie review dataset, that can be downloaded from [the Stanford page of Andrew Maas](https://ai.stanford.edu/~amaas/data/sentiment/). The movie reviews are contained in two folders: one for training samples and one for testing. These folders contain additional subfolders \"pos\" and \"neg\" with samples corresponding to their sentiment; note that the train directory contains an extra subfolder \"unsup\", which is unnecessary for this purpose and should be removed before continuing.   "
   ],
   "id": "21103dac0ccc2f7e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T09:17:19.690866Z",
     "start_time": "2025-11-13T09:16:54.312945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import keras\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = keras.utils.text_dataset_from_directory( \n",
    "    '../../aclImdb/train/', \n",
    "    validation_split=0.2, \n",
    "    subset=\"training\", \n",
    "    seed=123,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "val_ds = keras.utils.text_dataset_from_directory( \n",
    "    '../../aclImdb/train/', \n",
    "    validation_split=0.2, \n",
    "    subset=\"validation\", \n",
    "    seed=123, # same seed as above!\n",
    "    batch_size=batch_size)\n",
    "\n",
    "test_ds = keras.utils.text_dataset_from_directory( \n",
    "    '../../aclImdb/test/', \n",
    "    batch_size=batch_size)"
   ],
   "id": "cbd0c5d8dd6d0208",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Below we take a look at one of the training samples. The integer-valued labels are binary: either zero (negative sentiment) or one (positive sentiment).",
   "id": "fde7f4e532c0845"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T09:17:53.029713Z",
     "start_time": "2025-11-13T09:17:52.880387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print(inputs.shape, targets.shape)\n",
    "    print(inputs.dtype, targets.dtype)\n",
    "    print(inputs[0])\n",
    "    print(targets[0])\n",
    "    break"
   ],
   "id": "1a1112abd1c3b5b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32,) (32,)\n",
      "<dtype: 'string'> <dtype: 'int32'>\n",
      "tf.Tensor(b'After, I watched the films... I thought, \"Why the heck was this film such a high success in the Korean Box Office?\" Even thought the movie had a clever/unusal scenario, the acting wasn\\'t that good and the characters weren\\'t very interesting. For a Korean movie... I liked the fighting scenes. If you want to watch a film without thinking, this is the film for you. But I got to admit... the film was kind of childish... 6/10', shape=(), dtype=string)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For basic text preprocessing, Keras offers a `TextVectorization` layer. This layer can be used to preprocess the string-formed text data into numerical vectors. The code cell below does the following:\n",
    "\n",
    "* transforms the text to lowercase\n",
    "* removes punctuation\n",
    "* does word-level tokenization by splitting on whitespace\n",
    "* sets up a vocabulary of tokens\n",
    "* outputs the text converted to a vector of zeroes and ones.  "
   ],
   "id": "a6bf2f7f1970ce96"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T09:18:12.667181Z",
     "start_time": "2025-11-13T09:18:07.060179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from keras.layers import TextVectorization\n",
    "\n",
    "max_tokens = 10000 # Maximum vocabulary size \n",
    "\n",
    "vectorization_layer = TextVectorization( \n",
    "    max_tokens=max_tokens, \n",
    "    output_mode='multi_hot'\n",
    ")\n",
    "\n",
    "# Adapt the layer to the text data \n",
    "train_texts = train_ds.map(lambda x, y: x) \n",
    "vectorization_layer.adapt(train_texts)\n",
    "\n",
    "# Apply the vectorization to the datasets \n",
    "train_ds_bow = train_ds.map(lambda x, y: (vectorization_layer(x), y)) \n",
    "val_ds_bow = val_ds.map(lambda x, y: (vectorization_layer(x), y))\n",
    "test_ds_bow = test_ds.map(lambda x, y: (vectorization_layer(x), y))"
   ],
   "id": "d884f76f661f1418",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After this, the samples drawn from the Datasets are no longer strings, but vectors with as many elements as there are tokens in the vocabulary; in our example, we restrict the vocabulary size to 10000 most commonly encountered tokens in the training texts. Each of the vector elements are either one or zero depending on whether the text contained that particular token or not. ",
   "id": "9b3bb47fd2382f1b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T09:18:21.115983Z",
     "start_time": "2025-11-13T09:18:21.017783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for inputs, targets in train_ds_bow:\n",
    "    print(inputs.shape, targets.shape)\n",
    "    print(inputs.dtype, targets.dtype)\n",
    "    print(inputs[0])\n",
    "    print(targets[0])\n",
    "    break"
   ],
   "id": "ca56c417cc859d11",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 10000) (32,)\n",
      "<dtype: 'int64'> <dtype: 'int32'>\n",
      "tf.Tensor([1 1 1 ... 0 0 0], shape=(10000,), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This is a very simple example of a *bag-of-words* approach to natural language processing: the word vector merely indicates the presence or absence of a set of words in the input text. Let us now build and train a very simple fully connected classifier for the sentiment analysis task:",
   "id": "5bf0bdc603e1a8b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T09:19:58.552162Z",
     "start_time": "2025-11-13T09:18:52.343951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(max_tokens,)),\n",
    "    Dense(units=16, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_ds_bow, epochs=10, validation_data=val_ds_bow)"
   ],
   "id": "2f72a99b29c21124",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001B[1m625/625\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 12ms/step - accuracy: 0.8247 - loss: 0.4112 - val_accuracy: 0.8860 - val_loss: 0.2866\n",
      "Epoch 2/10\n",
      "\u001B[1m625/625\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 9ms/step - accuracy: 0.8922 - loss: 0.2849 - val_accuracy: 0.8860 - val_loss: 0.2970\n",
      "Epoch 3/10\n",
      "\u001B[1m625/625\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 8ms/step - accuracy: 0.9109 - loss: 0.2511 - val_accuracy: 0.8866 - val_loss: 0.3051\n",
      "Epoch 4/10\n",
      "\u001B[1m625/625\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 8ms/step - accuracy: 0.9166 - loss: 0.2436 - val_accuracy: 0.8898 - val_loss: 0.3145\n",
      "Epoch 5/10\n",
      "\u001B[1m625/625\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 9ms/step - accuracy: 0.9226 - loss: 0.2321 - val_accuracy: 0.8880 - val_loss: 0.3361\n",
      "Epoch 6/10\n",
      "\u001B[1m625/625\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 9ms/step - accuracy: 0.9269 - loss: 0.2246 - val_accuracy: 0.8892 - val_loss: 0.3457\n",
      "Epoch 7/10\n",
      "\u001B[1m625/625\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 9ms/step - accuracy: 0.9294 - loss: 0.2213 - val_accuracy: 0.8902 - val_loss: 0.3416\n",
      "Epoch 8/10\n",
      "\u001B[1m625/625\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 12ms/step - accuracy: 0.9309 - loss: 0.2189 - val_accuracy: 0.8860 - val_loss: 0.3580\n",
      "Epoch 9/10\n",
      "\u001B[1m625/625\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 14ms/step - accuracy: 0.9300 - loss: 0.2197 - val_accuracy: 0.8892 - val_loss: 0.3611\n",
      "Epoch 10/10\n",
      "\u001B[1m625/625\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 13ms/step - accuracy: 0.9315 - loss: 0.2157 - val_accuracy: 0.8868 - val_loss: 0.3566\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1f588aba550>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let us test the classifier:",
   "id": "d11a3c57cc2a1c56"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T09:20:32.770164Z",
     "start_time": "2025-11-13T09:20:28.392124Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Test accuracy: {model.evaluate(test_ds_bow)[1]:.4f}\")",
   "id": "540f1ee4068d5a3b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m782/782\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 5ms/step - accuracy: 0.8789 - loss: 0.3582\n",
      "Test accuracy: 0.8789\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can also inspect the vocabulary formed during the text vectorization process. The first element ([UNK]) of the corresponding dictionary is reserved for unknown words not contained in the vocabulary.",
   "id": "f4a614fe286c57a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T09:21:07.534292Z",
     "start_time": "2025-11-13T09:21:07.501215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocabulary = vectorization_layer.get_vocabulary()\n",
    "print(vocabulary[0:20]) # print 20 most common tokens"
   ],
   "id": "c7b74bcc4226dbea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', np.str_('the'), np.str_('and'), np.str_('a'), np.str_('of'), np.str_('to'), np.str_('is'), np.str_('in'), np.str_('it'), np.str_('i'), np.str_('this'), np.str_('that'), np.str_('br'), np.str_('was'), np.str_('as'), np.str_('with'), np.str_('for'), np.str_('movie'), np.str_('but'), np.str_('film')]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Word embeddings\n",
    "\n",
    "Representing texts as multi-hot vectors of zeroes and ones fails to take into account many aspects of natural language: the order of words in the text, the semantic relationships between different words and so on. Also, the vector representation is very sparse (the overwhelming majority of the vector elements are zero) and wasteful. A much better and useful alternative is to use dense encodings of floating-point numbers with smaller dimensionality. This approach is called **word embeddings**.\n",
    "\n",
    "With word embeddings, each word/token is associated with a vector in a small-dimensional (compared to the vocabulary size) embedding space. The components of these vectors are first initialized randomly, and then treated as trainable parameters. During training, the floating-point values of these vector components change iteratively, as they gather information about the dataset, and the relationships between words. \n",
    "\n",
    "In Keras, the word vector components are arranged in a special `Embedding` layer, as shown in the example implementation below. First, we create a new vectorization layer, which now outputs texts as lists of integer-valued token indices, and adapt this to the training texts. Since the text samples are of varying length, we define a `sequence_length` variable to restrict all lists to the same length (either by truncating long samples, or padding the short ones with zeroes)."
   ],
   "id": "986c23f303304ff3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T09:22:01.186457Z",
     "start_time": "2025-11-13T09:21:55.832306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the TextVectorization layer \n",
    "max_tokens = 10000 # Maximum vocabulary size \n",
    "sequence_length = 250 # Maximum sequence length \n",
    "\n",
    "vectorization_layer = TextVectorization( \n",
    "    max_tokens=max_tokens, \n",
    "    output_mode='int', \n",
    "    output_sequence_length=sequence_length )\n",
    "\n",
    "train_texts = train_ds.map(lambda x, y: x) \n",
    "vectorization_layer.adapt(train_texts)\n",
    "\n",
    "train_ds_int = train_ds.map(lambda x, y: (vectorization_layer(x), y)) \n",
    "val_ds_int = val_ds.map(lambda x, y: (vectorization_layer(x), y))\n",
    "test_ds_int = test_ds.map(lambda x, y: (vectorization_layer(x), y))"
   ],
   "id": "739a0440891dc282",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This is how the preprocessed samples now appear: ",
   "id": "9fe95bd700c2bd3e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T09:22:06.988924Z",
     "start_time": "2025-11-13T09:22:06.918604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for inputs, targets in train_ds_int:\n",
    "    print(inputs.shape, targets.shape)\n",
    "    print(inputs.dtype, targets.dtype)\n",
    "    print(inputs[0])\n",
    "    print(targets[0])\n",
    "    break"
   ],
   "id": "d867ff60a141d101",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 250) (32,)\n",
      "<dtype: 'int64'> <dtype: 'int32'>\n",
      "tf.Tensor(\n",
      "[  10  255   11    7    2   88  968   20 4574 1206   44   91    6 1315\n",
      "   11   20  139   26   75  629    8 2180   17    2 1081    6  656   21\n",
      "    2  200  288    9    7 1424   21  394   38   94  238   23  162   17\n",
      "    9    3  824    9   46 1383    1  116    4   85  283   16    2  447\n",
      "    3    2  869    2  346   37 1547 7555 2639 3245    3    1    1   24\n",
      " 3676    4 1006    5    2  517    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0], shape=(250,), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now that we have preprocessed our text samples to integer lists of fixed length, they can be directly inserted into a Keras Embedding layer, which converts the integer lists to floating-point valued tensors of shape (sequence length, embedding dimension). \n",
    "\n",
    "As can be seen in the cell below, the two-dimensional tensors from the Embedding layer are first converted to one-dimensional ones with shape (sequence length$\\cdot$embedding dimension, ) using a Reshape layer. This is then followed up by a fully connected classifier consisting of two Dense layers: one hidden layer plus the final output layer. The Dropout layers are added to reduce overfitting, and the model is trained for ten epochs.      "
   ],
   "id": "8dc3539053c98b45"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T09:24:19.150241Z",
     "start_time": "2025-11-13T09:22:33.337129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from keras.layers import Embedding, Dropout, Reshape\n",
    "\n",
    "embed_dim = 64 # dimension of the word embeddings\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(sequence_length,)),\n",
    "    Embedding(input_dim=max_tokens, output_dim=embed_dim),\n",
    "    Reshape((sequence_length * embed_dim,)),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.fit(train_ds_int, epochs=10, validation_data=val_ds_int)"
   ],
   "id": "2113140ef45dea87",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential_1\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001B[38;5;33mEmbedding\u001B[0m)           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m250\u001B[0m, \u001B[38;5;34m64\u001B[0m)        │       \u001B[38;5;34m640,000\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape (\u001B[38;5;33mReshape\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m16000\u001B[0m)          │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001B[38;5;33mDropout\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m16000\u001B[0m)          │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m32\u001B[0m)             │       \u001B[38;5;34m512,032\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001B[38;5;33mDropout\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m32\u001B[0m)             │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)              │            \u001B[38;5;34m33\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">640,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16000</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16000</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">512,032</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m1,152,065\u001B[0m (4.39 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,152,065</span> (4.39 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m1,152,065\u001B[0m (4.39 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,152,065</span> (4.39 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001B[1m625/625\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 13ms/step - accuracy: 0.6701 - loss: 0.5813 - val_accuracy: 0.8228 - val_loss: 0.3930\n",
      "Epoch 2/10\n",
      "\u001B[1m625/625\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 14ms/step - accuracy: 0.8611 - loss: 0.3328 - val_accuracy: 0.8428 - val_loss: 0.3668\n",
      "Epoch 3/10\n",
      "\u001B[1m625/625\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 13ms/step - accuracy: 0.9218 - loss: 0.2053 - val_accuracy: 0.8418 - val_loss: 0.4014\n",
      "Epoch 4/10\n",
      "\u001B[1m625/625\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 14ms/step - accuracy: 0.9513 - loss: 0.1296 - val_accuracy: 0.8426 - val_loss: 0.4805\n",
      "Epoch 5/10\n",
      "\u001B[1m625/625\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 14ms/step - accuracy: 0.9711 - loss: 0.0832 - val_accuracy: 0.8372 - val_loss: 0.5687\n",
      "Epoch 6/10\n",
      "\u001B[1m625/625\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 18ms/step - accuracy: 0.9775 - loss: 0.0659 - val_accuracy: 0.8346 - val_loss: 0.6207\n",
      "Epoch 7/10\n",
      "\u001B[1m625/625\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 21ms/step - accuracy: 0.9819 - loss: 0.0534 - val_accuracy: 0.8352 - val_loss: 0.6848\n",
      "Epoch 8/10\n",
      "\u001B[1m625/625\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 20ms/step - accuracy: 0.9851 - loss: 0.0429 - val_accuracy: 0.8404 - val_loss: 0.7189\n",
      "Epoch 9/10\n",
      "\u001B[1m625/625\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 20ms/step - accuracy: 0.9868 - loss: 0.0383 - val_accuracy: 0.8284 - val_loss: 0.8884\n",
      "Epoch 10/10\n",
      "\u001B[1m625/625\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 19ms/step - accuracy: 0.9887 - loss: 0.0371 - val_accuracy: 0.8262 - val_loss: 0.9048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1f58a522550>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The validation accuracy has not improved from the much simpler bag-of-words model, which is not very surprising: sentiments in the text can be expected to be reasonably well determined by merely looking at which kinds of words it contains. However, it is easy to deduce that with problems that require deeper insights into the subtle meanings of written text, it is often necessary to use models that are able to preserve the word orderings. \n",
    "\n",
    "Also, it is interesting to see that, after training our model, the word vectors in the Embedding layer encode semantic information about the relationships between words. To see that, first we extract the word vector components from the embedding layer, and construct the dictionaries from the vocabulary: "
   ],
   "id": "73ab87271ecf059d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T09:25:07.917885Z",
     "start_time": "2025-11-13T09:25:07.883292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract the embedding weights from the trained model \n",
    "embedding_layer = model.layers[0] # the first layer of the model\n",
    "embedding_weights = embedding_layer.get_weights()[0] # shape (10000, 64)\n",
    "\n",
    "# Get the word index from the tokenizer \n",
    "vocabulary = vectorization_layer.get_vocabulary() \n",
    "index_word = {idx: word for idx, word in enumerate(vocabulary)}\n",
    "word_index = {word: idx for idx, word in enumerate(vocabulary)}"
   ],
   "id": "c9c7074ef75ed337",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The following code snippet defines a function that takes a word as an input, and compares its word vector representation to those of all other words in the dictionary in turn, using cosine similarity (essentially the same as taking the dot product of the two vectors). The function then outputs the words with most similar representation.  ",
   "id": "d930677b8394129e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T09:26:00.636337Z",
     "start_time": "2025-11-13T09:26:00.338457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "\n",
    "# Function to find similar words \n",
    "def find_similar_words(target_word, top_n=10): \n",
    "    if target_word not in word_index: \n",
    "        return f'Word \"{target_word}\" not in vocabulary' \n",
    "    target_idx = word_index[target_word] \n",
    "    target_embedding = embedding_weights[target_idx].reshape(1, -1) # shape (1, 64)\n",
    "    similarities = cosine_similarity(embedding_weights, target_embedding).reshape(-1) # shape (10000,)\n",
    "    similar_indices = np.argsort(similarities)[-top_n-1:-1][::-1] \n",
    "    similar_words = [index_word[idx] for idx in similar_indices] \n",
    "    return similar_words \n",
    "    \n",
    "word_to_check = 'bad'\n",
    "print(f'Words similar to \"{word_to_check}\": {find_similar_words(word_to_check)}')"
   ],
   "id": "ca77d6f166c0f35e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to \"bad\": [np.str_('pathetic'), np.str_('dull'), np.str_('awful'), np.str_('waste'), np.str_('pointless'), np.str_('laughable'), np.str_('worst'), np.str_('boring'), np.str_('dreadful'), np.str_('worse')]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "From this output, it is obvious that the model has been able to figure out something about how the words resemble each other.",
   "id": "14f0573f28300c4e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
