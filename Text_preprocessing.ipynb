{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4. Text data and preprocessing\n",
    "\n",
    "Most of the recent impressive advances in machine learning and artificial intelligence have emerged in the field of natural language processing. In addition to simpler tasks such as sentiment analysis or text classification, there has been considerable progress in more advanced tasks such as translation and question answering. Text processing involves several unique techniques and methods, which we shall look at next.\n",
    "\n",
    "## Bag-of-words approach\n",
    "\n",
    "In natural language processing tasks, the original data is typically in the form of strings, or a list of strings. Neural networks cannot process strings directly, so they need to be converted to arrays consisting of numerical values. This preprocessing usually consists of a succession of distinct steps:\n",
    "\n",
    "* *Text standardization*: convert everything to lowercase, remove punctuation and other special characters, etc.\n",
    "* *Tokenization*: split the text into separate units or **tokens** (words, characters, N-grams ...), and set up a vocabulary\n",
    "* *Vectorization*: associate a numerical vector with each of the tokens in the vocabulary\n",
    "\n",
    "To investigate an example of implementing these methods, we use the well-known IMDB movie review dataset, that can be downloaded from [the Stanford page of Andrew Maas](https://ai.stanford.edu/~amaas/data/sentiment/). The movie reviews are contained in two folders: one for training samples and one for testing. These folders contain additional subfolders \"pos\" and \"neg\" with samples corresponding to their sentiment; note that the train directory contains an extra subfolder \"unsup\", which is unnecessary for this purpose and should be removed before continuing.   "
   ],
   "id": "21103dac0ccc2f7e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T09:12:46.155423Z",
     "start_time": "2024-11-13T09:12:29.050237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = keras.utils.text_dataset_from_directory( \n",
    "    '../../aclImdb/train/', \n",
    "    validation_split=0.2, \n",
    "    subset=\"training\", \n",
    "    seed=123,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "val_ds = keras.utils.text_dataset_from_directory( \n",
    "    '../../aclImdb/train/', \n",
    "    validation_split=0.2, \n",
    "    subset=\"validation\", \n",
    "    seed=123, # same seed as above!\n",
    "    batch_size=batch_size)\n",
    "\n",
    "test_ds = keras.utils.text_dataset_from_directory( \n",
    "    '../../aclImdb/test/', \n",
    "    batch_size=batch_size)"
   ],
   "id": "cbd0c5d8dd6d0208",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Below we take a look at one of the training samples. The integer-valued labels are binary: either zero (negative sentiment) or one (positive sentiment).",
   "id": "fde7f4e532c0845"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T09:12:57.582413Z",
     "start_time": "2024-11-13T09:12:57.526906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print(inputs.shape, targets.shape)\n",
    "    print(inputs.dtype, targets.dtype)\n",
    "    print(inputs[0])\n",
    "    print(targets[0])\n",
    "    break"
   ],
   "id": "1a1112abd1c3b5b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32,) (32,)\n",
      "<dtype: 'string'> <dtype: 'int32'>\n",
      "tf.Tensor(b'After, I watched the films... I thought, \"Why the heck was this film such a high success in the Korean Box Office?\" Even thought the movie had a clever/unusal scenario, the acting wasn\\'t that good and the characters weren\\'t very interesting. For a Korean movie... I liked the fighting scenes. If you want to watch a film without thinking, this is the film for you. But I got to admit... the film was kind of childish... 6/10', shape=(), dtype=string)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For basic text preprocessing, Keras offers a `TextVectorization` layer. This layer can be used to preprocess the string-formed text data into numerical vectors. The code cell below does the following:\n",
    "\n",
    "* transforms the text to lowercase\n",
    "* removes punctuation\n",
    "* does word-level tokenization by splitting on whitespace\n",
    "* sets up a vocabulary of tokens\n",
    "* outputs the text converted to a vector of zeroes and ones.  "
   ],
   "id": "a6bf2f7f1970ce96"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T09:13:16.980933Z",
     "start_time": "2024-11-13T09:13:11.899891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "max_tokens = 10000 # Maximum vocabulary size \n",
    "\n",
    "vectorization_layer = TextVectorization( \n",
    "    max_tokens=max_tokens, \n",
    "    output_mode='multi_hot'\n",
    ")\n",
    "\n",
    "# Adapt the layer to the text data \n",
    "train_texts = train_ds.map(lambda x, y: x) \n",
    "vectorization_layer.adapt(train_texts)\n",
    "\n",
    "# Apply the vectorization to the datasets \n",
    "train_ds_bow = train_ds.map(lambda x, y: (vectorization_layer(x), y)) \n",
    "val_ds_bow = val_ds.map(lambda x, y: (vectorization_layer(x), y))\n",
    "test_ds_bow = test_ds.map(lambda x, y: (vectorization_layer(x), y))"
   ],
   "id": "d884f76f661f1418",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After this, the samples drawn from the Datasets are no longer strings, but vectors with as many elements as there are tokens in the vocabulary; in our example, we restrict the vocabulary size to 10000 most commonly encountered tokens in the training texts. Each of the vector elements are either one or zero depending on whether the text contained that particular token or not. ",
   "id": "9b3bb47fd2382f1b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T09:14:13.515526Z",
     "start_time": "2024-11-13T09:14:13.440440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for inputs, targets in train_ds_bow:\n",
    "    print(inputs.shape, targets.shape)\n",
    "    print(inputs.dtype, targets.dtype)\n",
    "    print(inputs[0])\n",
    "    print(targets[0])\n",
    "    break"
   ],
   "id": "ca56c417cc859d11",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 10000) (32,)\n",
      "<dtype: 'int64'> <dtype: 'int32'>\n",
      "tf.Tensor([1 1 1 ... 0 0 0], shape=(10000,), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This is a very simple example of a *bag-of-words* approach to natural language processing: the word vector merely indicates the presence or absence of a set of words in the input text. Let us now build and train a very simple fully connected classifier for the sentiment analysis task:",
   "id": "5bf0bdc603e1a8b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T09:15:36.643760Z",
     "start_time": "2024-11-13T09:14:48.667072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(max_tokens,)),\n",
    "    Dense(units=16, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_ds_bow, epochs=10, validation_data=val_ds_bow)"
   ],
   "id": "2f72a99b29c21124",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001B[1m625/625\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 8ms/step - accuracy: 0.7592 - loss: 0.5025 - val_accuracy: 0.8780 - val_loss: 0.2982\n",
      "Epoch 2/10\n",
      "\u001B[1m625/625\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 7ms/step - accuracy: 0.8856 - loss: 0.3027 - val_accuracy: 0.8836 - val_loss: 0.2874\n",
      "Epoch 3/10\n",
      "\u001B[1m625/625\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 8ms/step - accuracy: 0.9051 - loss: 0.2710 - val_accuracy: 0.8844 - val_loss: 0.3011\n",
      "Epoch 4/10\n",
      "\u001B[1m625/625\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 7ms/step - accuracy: 0.9103 - loss: 0.2472 - val_accuracy: 0.8888 - val_loss: 0.3112\n",
      "Epoch 5/10\n",
      "\u001B[1m625/625\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 7ms/step - accuracy: 0.9169 - loss: 0.2461 - val_accuracy: 0.8848 - val_loss: 0.3262\n",
      "Epoch 6/10\n",
      "\u001B[1m625/625\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 8ms/step - accuracy: 0.9211 - loss: 0.2366 - val_accuracy: 0.8866 - val_loss: 0.3313\n",
      "Epoch 7/10\n",
      "\u001B[1m625/625\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 8ms/step - accuracy: 0.9231 - loss: 0.2355 - val_accuracy: 0.8856 - val_loss: 0.3429\n",
      "Epoch 8/10\n",
      "\u001B[1m625/625\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 7ms/step - accuracy: 0.9250 - loss: 0.2408 - val_accuracy: 0.8754 - val_loss: 0.3633\n",
      "Epoch 9/10\n",
      "\u001B[1m625/625\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 7ms/step - accuracy: 0.9266 - loss: 0.2287 - val_accuracy: 0.8732 - val_loss: 0.3727\n",
      "Epoch 10/10\n",
      "\u001B[1m625/625\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 7ms/step - accuracy: 0.9288 - loss: 0.2217 - val_accuracy: 0.8734 - val_loss: 0.3802\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let us test the classifier:",
   "id": "d11a3c57cc2a1c56"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T09:15:56.657021Z",
     "start_time": "2024-11-13T09:15:52.054314Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Test accuracy: {model.evaluate(test_ds_bow)[1]:.4f}\")",
   "id": "540f1ee4068d5a3b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m782/782\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 6ms/step - accuracy: 0.8669 - loss: 0.3832\n",
      "Test accuracy: 0.8708\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can also inspect the vocabulary formed during the text vectorization process. The first element ([UNK]) of the corresponding dictionary is reserved for unknown words not contained in the training texts. ",
   "id": "f4a614fe286c57a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T09:16:20.723625Z",
     "start_time": "2024-11-13T09:16:20.693977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocabulary = vectorization_layer.get_vocabulary()\n",
    "print(vocabulary[0:20]) # print 20 most common tokens"
   ],
   "id": "c7b74bcc4226dbea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i', 'this', 'that', 'br', 'was', 'as', 'with', 'for', 'movie', 'but', 'film']\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Word embeddings\n",
    "\n",
    "Representing texts as multi-hot vectors of zeroes and ones fails to take into account many aspects of natural language: the order of words in the text, the semantic relationships between different words and so on. Also, the vector representation is very sparse (the overwhelming majority of the vector elements are zero) and wasteful. A much better and useful alternative is to use dense encodings of floating-point numbers with smaller dimensionality. This approach is called **word embeddings**.\n",
    "\n",
    "With word embeddings, each word/token is associated with a vector in a small-dimensional (compared to the vocabulary size) embedding space. The components of these vectors are first initialized randomly, and then treated as trainable parameters. During training, the floating-point values of these vector components change iteratively, as they gather information about the dataset, and the relationships between words. \n",
    "\n",
    "In Keras, the word vector components are arranged in a special `Embedding` layer, as shown in the example implementation below. First, we create a new vectorization layer, which now outputs texts as lists of integer-valued token indices, and adapt this to the training texts. Since the text samples are of varying length, we define a `sequence_length` variable to restrict all lists to the same length (either by truncating long samples, or padding the short ones with zeroes)."
   ],
   "id": "986c23f303304ff3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T09:17:27.313467Z",
     "start_time": "2024-11-13T09:17:22.346858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the TextVectorization layer \n",
    "max_tokens = 10000 # Maximum vocabulary size \n",
    "sequence_length = 250 # Maximum sequence length \n",
    "\n",
    "vectorization_layer = TextVectorization( \n",
    "    max_tokens=max_tokens, \n",
    "    output_mode='int', \n",
    "    output_sequence_length=sequence_length )\n",
    "\n",
    "train_texts = train_ds.map(lambda x, y: x) \n",
    "vectorization_layer.adapt(train_texts)\n",
    "\n",
    "train_ds_int = train_ds.map(lambda x, y: (vectorization_layer(x), y)) \n",
    "val_ds_int = val_ds.map(lambda x, y: (vectorization_layer(x), y))\n",
    "test_ds_int = test_ds.map(lambda x, y: (vectorization_layer(x), y))"
   ],
   "id": "739a0440891dc282",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This is how the preprocessed samples now appear: ",
   "id": "9fe95bd700c2bd3e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T09:17:58.641603Z",
     "start_time": "2024-11-13T09:17:58.557604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for inputs, targets in train_ds_int:\n",
    "    print(inputs.shape, targets.shape)\n",
    "    print(inputs.dtype, targets.dtype)\n",
    "    print(inputs[0])\n",
    "    print(targets[0])\n",
    "    break"
   ],
   "id": "d867ff60a141d101",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 250) (32,)\n",
      "<dtype: 'int64'> <dtype: 'int32'>\n",
      "tf.Tensor(\n",
      "[  45   23  174    6   66    4   18   43  106 1192 6974  102   11    7\n",
      "    2   29    2  114    7  882  191   37  310 6183   15    2 1982 8703\n",
      "  905   15    2  652  260   35   63   24   46    6   77    3   37  847\n",
      " 4206   15   25 7975  115  471    6  905   28  492   17 2082  200 1339\n",
      "   19  233  487  191 8840    1    3    1    1    6 4624   30  266    6\n",
      " 1111  905    6 3560   17  325   60 2732    1  195   96   26 1738  242\n",
      "   16   11   29   10  196    9  215   12    8    4 4608 2606   16 1531\n",
      "  497    1 1912   21    2  289   28    1   12    2  106  414   68   53\n",
      "  269  956   43    2  230   12    2  172   14  738 1132    6  163   16\n",
      "   12    2  172    5 1323   68 1132 3920   91    5   89   12    2 1023\n",
      "   14 1316 1601   43  105   91    6  842  183 1061    3   38   21   11\n",
      " 1275    5   29  200   53 4908  225  254  614  676   21    2  288    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0], shape=(250,), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now that we have preprocessed our text samples to integer lists of fixed length, they can be directly inserted into a Keras Embedding layer, which converts the integer lists to floating-point valued tensors of shape (sequence length, embedding dimension). \n",
    "\n",
    "After this, one possibility would be to flatten the two-dimensional tensors to one-dimensional ones with shape (sequence length$\\cdot$embedding dimension, ), and follow up with Dense layers. However, such an approach would have two obvious drawbacks: first, the flattened vector would have tens of thousands of elements, which would cause trouble with overfitting. Second, and perhaps more importantly, such a model would process the entire sequence as a single whole, and would pay no attention whatsoever to the ordering of the words in the text samples. For these reasons, we attempt something slightly different.\n",
    "\n",
    "Keras offers a collection of **recurrent** layers, that are especially suited for treating samples with intrinsic ordering, such as time series data or, indeed, text in natural language. Crudely described, the recurrent layers process text samples as follows:\n",
    "\n",
    "* The word vector corresponding to the first word in the text sample enters the layer as its first input, and the layer computes an intermediate output.\n",
    "* The word vector corresponding to the second word enters the layer *together with* the previously computed intermediate output, and the second intermediate output is computed using *both of them*. \n",
    "* The process continues from one word vector to the next, while the information contained in the previous words is sustained in the calculations through the intermediate outputs.\n",
    "* The output computed after the last word vector in the sequence is the final output of the recurrent layer.\n",
    "\n",
    "In the example code cell below, we use a recurrent layer called **LSTM** (for \"Long short-term memory\"). The implementation is very easy, because LSTM layers accept the outputs from Embedding layer directly as their input, and so the only thing to define is the desired shape of the output vectors (this is given to the LSTM layer as a parameter).\n",
    "\n",
    "We add a Dropout layer before the final output layer to reduce overfitting, and train the model for five epochs (now the training takes quite a bit longer).      "
   ],
   "id": "8dc3539053c98b45"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T09:22:55.931680Z",
     "start_time": "2024-11-13T09:19:15.705309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.layers import Embedding, Dropout, LSTM\n",
    "\n",
    "embed_dim = 128 # dimension of the word embeddings\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(None,), dtype='int64'),\n",
    "    Embedding(input_dim=max_tokens, output_dim=embed_dim, mask_zero=True),\n",
    "    LSTM(64),\n",
    "    Dropout(0.5),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_ds_int, epochs=5, validation_data=val_ds_int)"
   ],
   "id": "2113140ef45dea87",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001B[1m625/625\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m42s\u001B[0m 64ms/step - accuracy: 0.6380 - loss: 0.6159 - val_accuracy: 0.8090 - val_loss: 0.4264\n",
      "Epoch 2/5\n",
      "\u001B[1m625/625\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m39s\u001B[0m 62ms/step - accuracy: 0.8317 - loss: 0.3971 - val_accuracy: 0.8544 - val_loss: 0.3438\n",
      "Epoch 3/5\n",
      "\u001B[1m625/625\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m49s\u001B[0m 78ms/step - accuracy: 0.8680 - loss: 0.3361 - val_accuracy: 0.8556 - val_loss: 0.3395\n",
      "Epoch 4/5\n",
      "\u001B[1m625/625\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m45s\u001B[0m 72ms/step - accuracy: 0.8896 - loss: 0.2860 - val_accuracy: 0.8666 - val_loss: 0.3552\n",
      "Epoch 5/5\n",
      "\u001B[1m625/625\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m46s\u001B[0m 73ms/step - accuracy: 0.9020 - loss: 0.2608 - val_accuracy: 0.8578 - val_loss: 0.3898\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The validation accuracy has not improved from the much simpler bag-of-words model, which is not very surprising: sentiments in the text can be expected to be reasonably well determined by merely looking at which kinds of words it contains. However, it is easy to deduce that with problems that require deeper insights into the subtle meanings of written text, it is often necessary to use models that are able to preserve the word orderings. \n",
    "\n",
    "Also, it is interesting to see that, after training our model, the word vectors in the Embedding layer encode semantic information about the relationships between words. To see that, first we extract the word vector components from the embedding layer, and construct the dictionaries from the vocabulary: "
   ],
   "id": "73ab87271ecf059d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T09:23:12.739458Z",
     "start_time": "2024-11-13T09:23:12.716043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract the embedding weights from the trained model \n",
    "embedding_layer = model.layers[0] \n",
    "embedding_weights = embedding_layer.get_weights()[0] # Shape: (vocab_size, embed_dim) \n",
    "\n",
    "# Get the word index from the tokenizer \n",
    "vocabulary = vectorization_layer.get_vocabulary() \n",
    "index_word = {idx: word for idx, word in enumerate(vocabulary)}\n",
    "word_index = {word: idx for idx, word in index_word.items()}"
   ],
   "id": "c9c7074ef75ed337",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The following code snippet defines a function that takes a word as an input, and compares its word vector representation to those of all other words in the dictionary in turn, using cosine similarity (essentially the same as taking the dot product of the two vectors). The function then outputs the words with most similar representation.  ",
   "id": "d930677b8394129e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T09:23:33.892216Z",
     "start_time": "2024-11-13T09:23:32.015801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "\n",
    "# Function to find similar words \n",
    " \n",
    "def find_similar_words(target_word, top_n=10): \n",
    "    if target_word not in word_index: \n",
    "        return f'Word \"{target_word}\" not in vocabulary' \n",
    "    target_idx = word_index[target_word] \n",
    "    target_embedding = embedding_weights[target_idx].reshape(1, -1) \n",
    "    similarities = cosine_similarity(embedding_weights, target_embedding).reshape(-1) \n",
    "    similar_indices = np.argsort(similarities)[-top_n-1:-1][::-1] \n",
    "    similar_words = [index_word[idx] for idx in similar_indices] \n",
    "    return similar_words \n",
    "    \n",
    "word_to_check = 'bad'\n",
    "print(f'Words similar to \"{word_to_check}\": {find_similar_words(word_to_check)}')"
   ],
   "id": "ca77d6f166c0f35e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to \"bad\": ['worse', 'stupid', 'waste', 'ridiculous', 'worst', 'terrible', 'poor', 'boring', 'supposed', 'disappointing']\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "From this output, it is obvious that the model has been able to figure out something about how the words resemble each other.",
   "id": "14f0573f28300c4e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "87f9ae811a226110"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
