{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. Introduction to neural networks\n",
    "\n",
    "## Minimal neural network\n",
    "\n",
    "Let us start by building a minimal neural network. First, run the following cell for the necessary imports (their significance will become clear later). You'll need to install the python package Tensorflow first using e.g. the Python Packages manager. With Tensorflow it is possible to access the Keras library, which specifies in creating neural networks of various types."
   ],
   "id": "88d9a12e62335f32"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T12:54:26.543034Z",
     "start_time": "2024-10-21T12:54:13.611846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical"
   ],
   "id": "ceaaa4d2",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Next, we download the famous MNIST dataset, which comes with Tensorflow and contains 28 x 28 pixel images of handwritten digits, labelled 0-9. The data ends up in four different NumPy tensors: `x_train` and `y_train` contain the input features and target labels for the training samples, and `x_test` and `y_test` the same for the test samples. \n",
    "\n",
    "The training set consists of 60000 image samples, and the test set a further 10000 samples."
   ],
   "id": "494d185388b339a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T13:22:28.234351Z",
     "start_time": "2024-10-21T13:22:28.028980Z"
    }
   },
   "cell_type": "code",
   "source": "(x_train, y_train), (x_test, y_test) = mnist.load_data()",
   "id": "2bce384a8ae093bc",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First, we do some basic preprocessing: the input features (pixel values) originally vary in the range 0 ... 255, and these values are divided by 255 to convert them to range 0 ... 1, which has computational advantages. Also, the labels are one-hot-encoded (with the Keras function `to_categorical`) from the original integer form; this ensures that the distances between two different labels are similar. For example, the label 4 is converted to the form [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]. ",
   "id": "267c1b36f68fb3d5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T13:22:31.432817Z",
     "start_time": "2024-10-21T13:22:31.300628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ],
   "id": "bfa58bb96856179a",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now we determine the structure (architecture) of our simple neural network. First we create an instance of class `Sequential`, which corresponds to a neural network consisting of layers stacked one after another (most of them are of this kind).\n",
    "\n",
    "This particular neural network model has three layers; ignore their details for now, as we'll look at them more closely below."
   ],
   "id": "4184bc750b25ae35"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T13:22:34.131781Z",
     "start_time": "2024-10-21T13:22:34.078336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(28, 28)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ],
   "id": "68e350acf47c9cd3",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next the model needs to be compiled, which means providing the learning algorithm some information about training and performance monitoring. Again, we'll look at the details more closely below. ",
   "id": "c8d1737e15ec06ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T13:22:36.299451Z",
     "start_time": "2024-10-21T13:22:36.268141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ],
   "id": "29cbc911881112d2",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now the model can be trained with the training data:",
   "id": "8d5b4ed51629d708"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T13:22:54.722617Z",
     "start_time": "2024-10-21T13:22:39.053807Z"
    }
   },
   "cell_type": "code",
   "source": "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)",
   "id": "600a97bb2ad2f0e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.2944 - accuracy: 0.9153 - val_loss: 0.1636 - val_accuracy: 0.9535\n",
      "Epoch 2/5\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.1330 - accuracy: 0.9615 - val_loss: 0.1254 - val_accuracy: 0.9626\n",
      "Epoch 3/5\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.0908 - accuracy: 0.9726 - val_loss: 0.1104 - val_accuracy: 0.9679\n",
      "Epoch 4/5\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.0679 - accuracy: 0.9799 - val_loss: 0.1023 - val_accuracy: 0.9703\n",
      "Epoch 5/5\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.0519 - accuracy: 0.9844 - val_loss: 0.0945 - val_accuracy: 0.9718\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1341b3fcc10>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now that our model has been trained, we can use it to predict the labels of the samples in the test set (which the model has so far not been exposed to), and evaluate the classification accuracy. We observe that, even this fairly simple model is able to predict the labels with accuracy that is above 97%.",
   "id": "a2510e5d738ee736"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T13:25:35.051772Z",
     "start_time": "2024-10-21T13:25:34.490826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f'Test accuracy: {test_acc}')"
   ],
   "id": "f876664c0cc638f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0804 - accuracy: 0.9759\n",
      "Test accuracy: 0.9758999943733215\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Workflow\n",
    "\n",
    "The simple example contains all the essential stages of deep learning experiments:\n",
    "\n",
    "* **Download the data**: prepare separate sets for training and testing as NumPy arrays\n",
    "* **Preprocess the data**: standardize/normalize input features, one-hot-encode categorical variables\n",
    "* **Build the model**: define the architecture and the various layers\n",
    "* **Compile the model**: give details concerning training and specify metrics\n",
    "* **Train the model**: allow the learning algorithm to find good parameter values for the model\n",
    "* **Evaluate the model**: test the performance with new, previously unseen data\n",
    "\n",
    "It is useful to think of a neural network as an information-processing unit. Input data enters the network in the form of a tensor, which is a multidimensional array of numbers. The first layer of the network modifies this tensor, and outputs a new one, which then enters the second layer as an input. The data then flows through the network from one layer to the next, and gets modified every step of the way, finally coming out as predictions for the target values. In other words, a neural network aims to model the connection between the inputs and outputs.\n",
    "\n",
    "In the example above, the first layer (the *Flatten* layer) only reshapes the input tensor. Originally, the shape of the tensor containing the input features in the training samples is "
   ],
   "id": "e08d813c79054fd3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T14:31:12.495972Z",
     "start_time": "2024-10-21T14:31:12.480271Z"
    }
   },
   "cell_type": "code",
   "source": "x_train.shape",
   "id": "48b658f5dcf21b6b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "That is, the three-dimensional input tensor contains 60000 samples, each determined by 28 times 28 pixel values: the first index determines the sample, and the second and third index point to the row and column of the collection of pixel values defining the image. The cell below shows one example of the input samples.",
   "id": "569a6cc40577facb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T14:26:30.764009Z",
     "start_time": "2024-10-21T14:26:30.699242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize a sample image\n",
    "sample_number = 42\n",
    "plt.imshow(x_train[sample_number], cmap='gray')\n",
    "plt.title(f'Label: {y_train[sample_number].argmax()}')  # Use .argmax() to get the label as an integer\n",
    "plt.axis('off')  # Hide the axes\n",
    "plt.show()"
   ],
   "id": "3e41cc8c7fd585b5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The first layer of the neural network model merely reshapes the input tensor from a three-dimensional to two-dimensional form: instead of a 2D table of 28 x 28 numerical values, each sample becomes a 1D vector of 28 x 28 = 784 numerical values. \n",
    "\n",
    "Following the Flatten layer, the above model has two additional *Dense* layers. These layers perform a somewhat more complex computation; this leads us to consider the mathematics of neural networks in more detail."
   ],
   "id": "5a4992c1afafeb29"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Single neuron\n",
    "\n",
    "Neural networks consist of a large number of similar unit entities, neurons, arranged in layers. Each of the neurons performs a similar kind of computation: it takes in a collection of input values, and produces a single number as an output. The computation takes place in two stages: 1) **weighted sum**, followed by 2) **activation function**.\n",
    "\n",
    "Assume that an individual neuron takes in N inputs: $x_{1}, x_{2}, ..., x_{N}$. Such a neuron has N parameters called **weights**, $w_{1}, w_{2}, ..., w_{N}$, and a **bias** parameter $b$. First, the neuron computes a weighted sum $z$ of the inputs as follows:\n",
    "\n",
    "$$\n",
    "z = w_{1}x_{1} + w_{2}x_{2} + ... + w_{N}x_{N} + b\n",
    "$$\n",
    "After this, the final output $y$ of the neuron is obtained from\n",
    "\n",
    "$$\n",
    "y = f(z)\n",
    "$$\n",
    "where the function $f$ is called an **activation function**. Common choices for activation functions are the sigmoid\n",
    "\n",
    "$$\n",
    "f(z) = \\frac{1}{e^{-x}+1}\n",
    "$$ \n",
    "and the ReLU (rectified linear unit)\n",
    "\n",
    "$$\n",
    "f(z) = \\begin{cases} \n",
    "z & \\hspace{1cm} z > 0 \\\\\n",
    "0 & \\hspace{1cm} \\text{otherwise} \n",
    "\\end{cases} \n",
    "$$\n",
    "The activation functions are typically chosen to be nonlinear (as the two examples above are) to increase the complexity of the computation performed by the neurons and, therefore, to be able to model complex behavior.\n",
    "\n",
    "### Fully connected (Dense) layer\n",
    "\n",
    "A **fully connected** layer (or *Dense* layers, as they are referred to in Keras) is simply a collection of several neurons, each having their individual set of weight and bias values. For example, the second layer in our model above has a total of 128 neurons. Each of these 128 neurons takes all 784 pixel values of each sample in as input; each neuron also has 128 weights and one bias value, and they compute a weighted sum of the 784 input features. Next, each of these 784 intermediate outputs are inserted in the activation function (this is the same for all 128 neurons in the layer), producing the final output of 128 values. In this way, the dense layer processes the original 1D vector of 784 values to another 1D vector of 128 values. This information then continues to the next layer as an input.\n",
    "\n",
    "![Alt text](Images/nn.svg)"
   ],
   "id": "5c8df2c03f7f518b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T14:33:37.859533Z",
     "start_time": "2024-10-21T14:33:37.821492Z"
    }
   },
   "cell_type": "code",
   "source": "model.summary()",
   "id": "7119328f77e175fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_3 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               100480    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "91ded2be9154262a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
